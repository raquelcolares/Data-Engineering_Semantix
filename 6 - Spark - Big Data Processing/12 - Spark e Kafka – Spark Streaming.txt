			Exercício 12 - Spark e Kafka – Spark Streaming


Kafka

1 - Preparação do ambiente no Kafka
a) Criar o tópico “topic-spark” com 1 partição e o fator de replicação = 1
$ docker exec -it kafka bash
# kafka-topics.sh --bootstrap-server kafka:9092 --topic topic-spark --create --partition 1 --replication-factor 1

b) Inserir as seguintes mensagens no tópico:
- Msg1
- Msg2
- Msg3
c) Criar um consumidor no Kafka para ler o “topic-spark”
# kafka-console-consumer.sh --bootsrap-server kafka:9092 --topic-spark
> Msg1
> Msg2
> Msg3




Spark

1 - Criar um consumidor em Scala usando Spark Streaming para ler o “topic-spark” no cluster Kafka ”kafka:9092”
$ docker exec -it jupyter-spark bash
# spark-shell --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.1
> import org.apache.kafka.clients.consumer.ConsumerRecord
> import org.apache.kafka.common.serialization.StringDeserializer
> import org.apache.spark.streaming.kafka010._
> import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
> import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe

> val kafkaParams = Map[String, Object](
    "bootstrap.servers" -> "kafka:9092",
    "key.deserializer" -> classOf[StringDeserializer],
    "value.deserializer" -> classOf[StringDeserializer],
    "group.id" -> "aplicacao1",
    "auto.offset.reset" -> "earliest",
    "enable.auto.commit" -> (false: java.lang.Boolean)
  )

> import org.apache.spark.st
> import org.apache.spark.streaming.{StreamingContext, Seconds}

> val ssc = new Stream
> val ssc = new StreamingContext(sc, Seconds(5))

> val topic = Array("topic-spark")
> val stream = KafkaUtils.createDirectStream[String, String](
    ssc,
    PreferConsistent,
    Subscribe[String, String](topic, kafkaParams)
  )

2 - Visualizar o tópico com as seguintes informações
Nome do tópico
Partição
Valor

> val info_stream = stream.map(record => (
  record.topic,
  record.partition,
  record.value
  ))

3 - Salvar o tópico no diretório hdfs://namenode:8020/user/<nome>/kafka/dstream
> info_stream.saveAsTextFiles("hdfs://namenode:8020/user/raquel/kafka/dstream")
> ssc.start()