			Exercício 9 - WithColumn 2


1 - Criar um dataframe para ler o arquivo no HDFS /user/<nome/data/juros_selic/juros_selic
juros_selic = spark.read.csv("/user/raquel/data/juros_selic/juros_selic", sep=";", header="true")

2 - Agrupar todas as datas pelo ano em ordem decrescente e salvar a quantidade de meses ocorridos, o valor médio, mínimo e máximo do campo valor com a seguinte estrutura:

Anual | Meses | Valor médio | Valor Mínimo | Valor Máximo |

2019  |   2   |    00.00    |    00.00     |    00.00     |

2018  |  12   |    00.00    |    00.00     |    00.00     |

2017  |  12   |    00.00    |    00.00     |    00.00     |

...   |  ...  |    00.00    |    00.00     |    00.00     |
  
1986  |   2   |    00.00    |    00.00     |    00.00     |

juros_ano = juros_selic.withColumn("ano", split(col("data"), "/").getItem(2)_
juros_valor = juros_ano.withColumn("valor", regexp_replace(col("valor"), "\", "\.").cast(FloatType()))

juros_relatorio = juros_valor.groupBy("ano").agg(count("ano")).alias("Meses"), format_number(avg("valor"),2).alias("Valor médio"), min("valor").alias("Valor mínimo"), max("valor").alias("Valor máximo")).sort(desc("ano")).show()

3 - Salvar no hdfs:///user/<nome>/relatorioAnual com compressão zlib e formato orc
juros_relatorio.write.orc("/user/raquel/relatorio_anual", compression="zlib")
