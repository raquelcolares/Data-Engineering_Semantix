			Exercício 13 - Spark e Kafka – Spark Streaming


Kafka

1 - Preparação do ambiente no Kafka
a) Criar o tópico “topic-kvspark” com 2 partições e o fator de replicação = 1
$ docker exec -it kafka bash
# kafka-topics.sh --bootstrap-server kafka:9092 --topic topic-kvspark --create --partition 2 --replication-factor 1
# kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic topic-kvspark

b) Inserir as seguintes mensagens no tópico (Chave, Valor):
- 1, Msg1
- 2, Msg2
- 3, Msg3
c) Criar um consumidor no Kafka para ler o “topic-kvspark”
$ docker exec -it kafka bash
# kafka-console-producer.sh --broker-list kafka:9092 --topic topic-kvspark --property parse.key=true --property key.separator=,
> 1,Msg1
> 2,Msg2
> 3,Msg3



Spark

1 - Criar um consumidor em Scala usando Spark Streaming para ler o “topic-kvspark” no cluster Kafka ”kafka:9092”
$ docker exec -it jupyter-spark bash
# spark-shell --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.1
> import org.apache.kafka.clients.consumer.ConsumerRecord
> import org.apache.kafka.common.serialization.StringDeserializer
> import org.apache.spark.streaming.kafka010._
> import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
> import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe

> val kafkaParams = Map[String, Object](
    "bootstrap.servers" -> "kafka:9092",
    "key.deserializer" -> classOf[StringDeserializer],
    "value.deserializer" -> classOf[StringDeserializer],
    "group.id" -> "aplicacao2",
    "auto.offset.reset" -> "earliest",
    "enable.auto.commit" -> (false: java.lang.Boolean)
  )

> val ssc = new StreamingContext(sc, Seconds(5))

> val topic = Array("topic-kvspark")
> val stream = KafkaUtils.createDirectStream[String, String](
    ssc,
    PreferConsistent,
    Subscribe[String, String](topic, kafkaParams)
  )

2 - Visualizar o tópico com as seguintes informações
Nome do tópico
Partição
Chave
Valor

> val info_stream = stream.map(record => (
  record.topic,
  record.partition,
  record.key,
  record.value
  ))

3 - Salvar o tópico no diretório hdfs://namenode:8020/user/<nome>/kafka/dstreamkv
> info_dstream.saveAsTextFiles("hdfs://namenode:8020/user/raquel/kafka/dstreamkv")
> ssc.start()